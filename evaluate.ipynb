{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Image Classification - Model Evaluation\n",
    "\n",
    "This notebook evaluates trained models on the test dataset and generates comprehensive performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import config\n",
    "from dataset import get_data_loaders\n",
    "from models import get_model\n",
    "from utils import (\n",
    "    set_seed, get_device, load_checkpoint,\n",
    "    plot_confusion_matrix, plot_roc_curves,\n",
    "    calculate_metrics, save_metrics\n",
    ")\n",
    "\n",
    "set_seed(config.RANDOM_SEED)\n",
    "device = get_device()\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data loaders\n",
    "train_loader, val_loader, test_loader, class_weights = get_data_loaders(\n",
    "    config.DATASET_DIR,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "class_names = config.CLASS_NAMES\n",
    "print(f'Class names: {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "Load and evaluate each trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, checkpoint_path):\n",
    "    \"\"\"Evaluate a single model\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = get_model(model_name, num_classes=config.NUM_CLASSES, pretrained=False)\n",
    "    checkpoint = load_checkpoint(checkpoint_path, model)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'labels': np.array(all_labels),\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'probabilities': np.array(all_probabilities),\n",
    "        'checkpoint': checkpoint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = {}\n",
    "\n",
    "models_to_evaluate = [\n",
    "    ('vgg16', os.path.join(config.CHECKPOINT_DIR, 'vgg16_best.pth')),\n",
    "    ('resnet50', os.path.join(config.CHECKPOINT_DIR, 'resnet50_best.pth')),\n",
    "    ('densenet121', os.path.join(config.CHECKPOINT_DIR, 'densenet121_best.pth'))\n",
    "]\n",
    "\n",
    "for model_name, checkpoint_path in models_to_evaluate:\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        results[model_name] = evaluate_model(model_name, checkpoint_path)\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint not found for {model_name} at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    val_acc = result['checkpoint'].get('best_val_acc', 'N/A')\n",
    "    test_acc = result['accuracy']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': model_name.upper(),\n",
    "        'Validation Acc (%)': f\"{val_acc:.2f}\" if isinstance(val_acc, float) else val_acc,\n",
    "        'Test Acc (%)': f\"{test_acc:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv(os.path.join(config.RESULTS_DIR, 'model_summary.csv'), index=False)\n",
    "print(f\"\\nSummary saved to {os.path.join(config.RESULTS_DIR, 'model_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Metrics for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name.upper()} - Detailed Classification Report\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        result['labels'], \n",
    "        result['predictions'], \n",
    "        target_names=class_names,\n",
    "        digits=4\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = calculate_metrics(result['labels'], result['predictions'], class_names)\n",
    "    metrics['test_accuracy'] = result['accuracy']\n",
    "    save_metrics(metrics, os.path.join(config.RESULTS_DIR, f\"{model_name}_metrics.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(18, 5))\n",
    "\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(result['labels'], result['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=axes[idx], cbar_kws={'label': 'Count'})\n",
    "    axes[idx].set_title(f'{model_name.upper()}\\nAccuracy: {result[\"accuracy\"]:.2f}%', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label', fontsize=12)\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.RESULTS_DIR, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "n_classes = len(class_names)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\nGenerating ROC curves for {model_name.upper()}...\")\n",
    "    \n",
    "    # Binarize labels\n",
    "    y_true_bin = label_binarize(result['labels'], classes=range(n_classes))\n",
    "    y_probs = result['probabilities']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, (class_name, color) in enumerate(zip(class_names, colors)):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=color, linewidth=2,\n",
    "                label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'{model_name.upper()} - ROC Curves', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, f'{model_name}_roc_curves.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare per-class F1-scores\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_to_plot = ['precision', 'recall', 'f1-score']\n",
    "titles = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for metric_idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    data = []\n",
    "    \n",
    "    for model_name in results.keys():\n",
    "        metrics = calculate_metrics(\n",
    "            results[model_name]['labels'], \n",
    "            results[model_name]['predictions'], \n",
    "            class_names\n",
    "        )\n",
    "        \n",
    "        for class_name in class_names:\n",
    "            data.append({\n",
    "                'Model': model_name.upper(),\n",
    "                'Class': class_name,\n",
    "                'Score': metrics['per_class_metrics'][class_name][metric]\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot grouped bar chart\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for idx, model_name in enumerate(results.keys()):\n",
    "        model_data = df[df['Model'] == model_name.upper()]\n",
    "        axes[metric_idx].bar(x + idx*width, model_data['Score'], width, \n",
    "                            label=model_name.upper())\n",
    "    \n",
    "    axes[metric_idx].set_xlabel('Class', fontsize=12)\n",
    "    axes[metric_idx].set_ylabel(title, fontsize=12)\n",
    "    axes[metric_idx].set_title(f'Per-Class {title}', fontsize=14, fontweight='bold')\n",
    "    axes[metric_idx].set_xticks(x + width)\n",
    "    axes[metric_idx].set_xticklabels(class_names, rotation=15, ha='right')\n",
    "    axes[metric_idx].legend()\n",
    "    axes[metric_idx].grid(True, alpha=0.3, axis='y')\n",
    "    axes[metric_idx].set_ylim([0, 1.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.RESULTS_DIR, 'per_class_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The evaluation is complete. Check the `results/` directory for saved metrics and visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}