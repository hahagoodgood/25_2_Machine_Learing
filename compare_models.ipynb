{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Classification - Model Comparison\n",
    "\n",
    "Compare VGG16, ResNet50, and DenseNet121 performance side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import config\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "models_to_compare = ['vgg16', 'resnet50', 'densenet121']\n",
    "histories = {}\n",
    "test_accs = {}\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'{model_name}_final.pth')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        histories[model_name] = checkpoint.get('history', {})\n",
    "        test_accs[model_name] = checkpoint.get('test_acc', None)\n",
    "        print(f\"Loaded {model_name}: Test Acc = {test_accs[model_name]:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint not found for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "colors = {'vgg16': 'blue', 'resnet50': 'red', 'densenet121': 'green'}\n",
    "markers = {'vgg16': 'o', 'resnet50': 's', 'densenet121': '^'} \n",
    "\n",
    "# Plot 1: Training Loss\n",
    "for model_name, history in histories.items():\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], \n",
    "                    color=colors[model_name], linewidth=2,\n",
    "                    marker=markers[model_name], markersize=4, markevery=5,\n",
    "                    label=model_name.upper())\n",
    "\n",
    "axes[0, 0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "for model_name, history in histories.items():\n",
    "    epochs = range(1, len(history['val_loss']) + 1)\n",
    "    axes[0, 1].plot(epochs, history['val_loss'], \n",
    "                    color=colors[model_name], linewidth=2,\n",
    "                    marker=markers[model_name], markersize=4, markevery=5,\n",
    "                    label=model_name.upper())\n",
    "\n",
    "axes[0, 1].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training Accuracy\n",
    "for model_name, history in histories.items():\n",
    "    epochs = range(1, len(history['train_acc']) + 1)\n",
    "    axes[1, 0].plot(epochs, history['train_acc'], \n",
    "                    color=colors[model_name], linewidth=2,\n",
    "                    marker=markers[model_name], markersize=4, markevery=5,\n",
    "                    label=model_name.upper())\n",
    "\n",
    "axes[1, 0].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Validation Accuracy\n",
    "for model_name, history in histories.items():\n",
    "    epochs = range(1, len(history['val_acc']) + 1)\n",
    "    axes[1, 1].plot(epochs, history['val_acc'], \n",
    "                    color=colors[model_name], linewidth=2,\n",
    "                    marker=markers[model_name], markersize=4, markevery=5,\n",
    "                    label=model_name.upper())\n",
    "\n",
    "axes[1, 1].set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.RESULTS_DIR, 'training_curves_comparison.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing test accuracies\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names_upper = [m.upper() for m in test_accs.keys()]\n",
    "accuracies = list(test_accs.values())\n",
    "bar_colors = [colors[m] for m in test_accs.keys()]\n",
    "\n",
    "bars = ax.bar(model_names_upper, accuracies, color=bar_colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison on Test Set', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 105])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.RESULTS_DIR, 'test_accuracy_comparison.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model = max(test_accs, key=test_accs.get)\n",
    "best_acc = test_accs[best_model]\n",
    "print(f\"\\n\ud83c\udfc6 Best performing model: {best_model.upper()} with {best_acc:.2f}% test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Epochs Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final training metrics\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, history in histories.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name.upper(),\n",
    "        'Final Train Acc (%)': f\"{history['train_acc'][-1]:.2f}\",\n",
    "        'Final Val Acc (%)': f\"{history['val_acc'][-1]:.2f}\",\n",
    "        'Test Acc (%)': f\"{test_accs[model_name]:.2f}\",\n",
    "        'Epochs Trained': len(history['train_acc'])\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(os.path.join(config.RESULTS_DIR, 'final_comparison.csv'), index=False)\n",
    "print(f\"\\nComparison saved to {os.path.join(config.RESULTS_DIR, 'final_comparison.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics from JSON files\n",
    "detailed_metrics = {}\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    metrics_path = os.path.join(config.RESULTS_DIR, f'{model_name}_metrics.json')\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            detailed_metrics[model_name] = json.load(f)\n",
    "        print(f\"Loaded metrics for {model_name}\")\n",
    "    else:\n",
    "        print(f\"Metrics file not found for {model_name}. Run evaluate.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Performance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detailed_metrics:\n",
    "    # Create heatmap of F1-scores\n",
    "    f1_data = []\n",
    "    \n",
    "    for model_name in models_to_compare:\n",
    "        if model_name in detailed_metrics:\n",
    "            model_f1 = []\n",
    "            for class_name in config.CLASS_NAMES:\n",
    "                f1 = detailed_metrics[model_name]['per_class_metrics'][class_name]['f1-score']\n",
    "                model_f1.append(f1)\n",
    "            f1_data.append(model_f1)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    f1_df = pd.DataFrame(f1_data, \n",
    "                        index=[m.upper() for m in models_to_compare if m in detailed_metrics],\n",
    "                        columns=config.CLASS_NAMES)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(f1_df, annot=True, fmt='.3f', cmap='YlGnBu', \n",
    "                cbar_kws={'label': 'F1-Score'}, vmin=0, vmax=1)\n",
    "    plt.title('Per-Class F1-Score Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Model', fontsize=12)\n",
    "    plt.xlabel('Class', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'f1_score_heatmap.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No detailed metrics available. Run evaluate.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time & Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training efficiency\n",
    "efficiency_data = []\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f'{model_name}_final.pth')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        training_time = checkpoint.get('training_time', 0)\n",
    "        test_acc = checkpoint.get('test_acc', 0)\n",
    "        \n",
    "        efficiency_data.append({\n",
    "            'Model': model_name.upper(),\n",
    "            'Training Time (min)': f\"{training_time / 60:.2f}\",\n",
    "            'Test Accuracy (%)': f\"{test_acc:.2f}\",\n",
    "            'Acc per Min': f\"{test_acc / (training_time / 60):.2f}\" if training_time > 0 else 'N/A'\n",
    "        })\n",
    "\n",
    "if efficiency_data:\n",
    "    efficiency_df = pd.DataFrame(efficiency_data)\n",
    "    print(\"\\nTraining Efficiency Comparison:\")\n",
    "    print(efficiency_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Training time information not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "Based on the comparison above:\n",
    "\n",
    "1. **Best Overall Model**: The model with highest test accuracy\n",
    "2. **Most Efficient**: Model with best accuracy-to-time ratio\n",
    "3. **Most Stable**: Model with smallest gap between train and validation accuracy\n",
    "\n",
    "Consider these factors when choosing a model for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}